# ELO rating experiments for problem solving

Given a collection of agents (of varying strengths) that attempt to solve a
collection of problems (of varying difficulty), one can try to rank the quality
of the agents. If agents experience downtime and don't attempt the exact same
set of problems, one can use a Bayesean model to infer some ELO-type rating for
the agents (and problems).

This project simulates this situation for experimentation with such rating
model. It consists of two python scripts

1.  `simulate.py`: Simulates a competition where agents attempt to solve problems. It outputs a JSON file detailing each attempt.
2.  `rate.py`: Takes the JSON output from `simulate.py` to rate the agents and problems based on the outcomes. It then outputs a JSON file containing these ratings.

## Rating model used

The rating model used is a Bayesian variant on chess ELO rating. It expresses
strength `s` of an agent and difficulty `d` of a problem on a numerical scale,
such that

> Probability of success = 1 / (1 + exp(d - s))

It assumes that strengths and difficulties are normally distributed (with
user-provided parameters), and that the strength of an agent is *constant*.

Given the outcomes of a competition, it uses a
numerical Newton scheme to find the maximum likelihood estimates of the
strengths of agents and difficulties of problems. In particular, this also gives
uncertainty quantification of the computed ratings.

Note: To convert `s` and `d` to regular chess ELO scale, multiply by 400/log(10) â‰ˆ 173.7
and add an arbitrary constant.

## Usage

### 1. Simulate Competition Data

The `simulate.py` script generates synthetic data of agents attempting problems.
The parameters are hard-coded in the file. They determine the distribution of

- the difficulties of the problems
- the strengths of the agents
- the amount of downtime of the agents

To run the simulation and generate an output file (e.g., `simulation_attempts.json`):
```bash
python simulate.py --output_file simulation_attempts.json
```
This will create `simulation_attempts.json` which encodes a list of attempts
formatted as:
```json
[
  {
    "agent": "Agent1",
    "problem": "Problem1",
    "outcome": "failed"
  },
  {
    "agent": "Agent2",
    "problem": "Problem1",
    "outcome": "solved"
  },
  // ...
]
```

Alternatively, one can produce such json from an external source of competition data.

### 2. Rate Agents and Problems

The `rate.py` script processes the data generated by `simulate.py` to infer
agent strengths and problem difficulties using a maximum likelihood estimation
for the Bradley-Terry model.


To rate the agents and problems using the output from the simulation step:
```bash
python rate.py simulation_attempts.json --output_file ratings.json
```
This command will read `simulation_attempts.json`, perform the rating calculations, and save the results to `ratings.json`.

