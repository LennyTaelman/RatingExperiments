# ELO rating experiments for problem solving

Given a collection of agents (of varying strengths) that attempt to solve a
collection of problems (of varying difficulty), one can try to rank the quality
of the agents. If agents experience downtime and don't attempt the exact same
set of problems, one can use a Bayesian model to infer some ELO-type rating for
the agents (and problems).

This project simulates this situation for experimentation with such rating
model. It consists of two python scripts

1.  `simulate.py`: Simulates a competition where agents attempt to solve problems. It outputs a JSON file detailing each attempt.
2.  `rate.py`: Takes the JSON output from `simulate.py` to rate the agents and problems based on the outcomes. It then outputs a JSON file containing these ratings.

## Rating model used

The rating model used is a Bayesian variant on chess ELO rating. The underlying
model is the Bradley--Terry model. It expresses strength `s` of an agent and
difficulty `d` of a problem on a numerical scale, such that

> Probability of agent solving the problem = 1 / (1 + exp(d - s))

Our rating assumes (prior) that strengths and difficulties are normally distributed (with
user-provided parameters). Given the outcomes of a competition, one can
numerically find maximum likelihood estimates and uncertainty quantification.
This implementation uses a simple Newton approximation.

Note: To convert `s` and `d` to regular chess ELO scale, multiply by 400/log(10) ≈ 173.7
and add an arbitrary constant.

## Usage

### 1. Simulate Competition Data

The `simulate.py` script generates synthetic data of agents attempting problems.
The parameters are hard-coded as global constants, to change them edit the file.
They determine the distribution of

- the difficulties of the problems
- the strengths of the agents
- the amount of downtime of the agents

To run the simulation and generate an output file (e.g.,
`simulation_attempts.json`):

```sh
python simulate.py --output_file simulation_attempts.json
```

This will create `simulation_attempts.json` which encodes a list of attempts
formatted as:

```json
[
  {
    "agent": "Agent1",
    "problem": "Problem1",
    "outcome": "failed"
  },
  {
    "agent": "Agent2",
    "problem": "Problem1",
    "outcome": "solved"
  },
  // ...
]
```

Alternatively, one can produce such json from an external source of competition data.

### 2. Rate Agents and Problems

The `rate.py` script processes the data generated by `simulate.py` to infer
agent strengths and problem difficulties using a maximum likelihood estimation
for the Bradley-Terry model.


To rate the agents and problems using the output from the simulation step:
```bash
python rate.py simulation_attempts.json --output_file ratings.json
```
This command will read `simulation_attempts.json`, perform the rating calculations, and save the results to `ratings.json`.


## Example results

This experimental repository is built to help inform choices in the [SorryDB
project](https://github.com/SorryDB/SorryDB). Below is one run of the experiment
with rather sparse data (weak signal).


### Simulation

We used as simulation parameters:

- 20 agents with downtime uniformally chosen between 0 and 0.8
- 1000 problems ('sorries')
- mean difficulty - mean strength = 15 (corresponding to a success rate of
  3e-07)
- std deviation of agent strength = 2.5
- std deviation of problem difficulty = 5

The outcome of the simulation consisted of

- 10984 attempts
- 20 problems were solved by at least one agent
- 13 agents solved at least one problem

### Inference

Maximum likelihood estimation excluding unsolved problems or agents with score
0. As prior we used:

- strengths and difficulties are ~ N(0,1)

### Inferred agent ratings

| Rank | ID   | Strength       | Solved | Attempts | Solve Rate |
|:----:|:----:|:--------------:|:------:|:--------:|:----------:|
| 1    | A12  | 1.07 ± 0.43    | 12     | 878      | 0.014      |
| 2    | A14  | 1.04 ± 0.57    | 5      | 324      | 0.015      |
| 3    | A20  | 0.87 ± 0.72    | 2      | 348      | 0.006      |
| 4    | A15  | 0.22 ± 0.47    | 6      | 754      | 0.008      |
| 5    | A10  | 0.13 ± 0.51    | 5      | 589      | 0.008      |
| 6    | A3   | -0.06 ± 0.69   | 1      | 216      | 0.005      |
| 7    | A5   | -0.14 ± 0.59   | 2      | 461      | 0.004      |
| 8    | A11  | -0.30 ± 0.69   | 1      | 385      | 0.003      |
| 9    | A18  | -0.41 ± 0.69   | 1      | 230      | 0.004      |
| 10   | A19  | -0.48 ± 0.57   | 2      | 563      | 0.004      |
| 11   | A4   | -0.57 ± 0.67   | 1      | 650      | 0.002      |
| 12   | A7   | -0.66 ± 0.57   | 2      | 660      | 0.003      |
| 13   | A6   | -0.71 ± 0.56   | 2      | 688      | 0.003      |

### Inferred problem ratings

| Rank | ID    | Difficulty     | Solved by                              |
|:----:|:-----:|:--------------:|----------------------------------------|
| 1    | P722  | 1.52 ± 0.63    | A10                                    |
| 2    | P539  | 1.48 ± 0.63    | A14                                    |
| 3    | P571  | 1.40 ± 0.66    | A14                                    |
| 4    | P916  | 1.37 ± 0.65    | A12                                    |
| 5    | P159  | 1.16 ± 0.69    | A7                                     |
| ...  | ...   | ...            | ...                                    |
| 16   | P495  | 0.18 ± 0.65    | A6, A15, A12                           |
| 17   | P825  | -0.01 ± 0.68   | A10, A15, A12                          |
| 18   | P183  | -0.03 ± 0.68   | A5, A12, A19                           |
| 19   | P628  | -0.34 ± 0.65   | A10, A11, A12, A19                     |
| 20   | P352  | -1.35 ± 0.66   | A10, A6, A7, A15, A18, A12, A4, A14    |
